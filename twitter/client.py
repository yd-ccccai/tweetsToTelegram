import requests
from bs4 import BeautifulSoup
import re
import random
import time
import json
import urllib3
import os
import shutil
from urllib.parse import urlparse
from config import Config
import cloudscraper  # æ·»åŠ  cloudscraper åº“

# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class TwitterClient:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Referer": "https://nitter.net/",  # æ·»åŠ  Referer
            "Origin": "https://nitter.net"     # æ·»åŠ  Origin
        }
        self.delay_range = (2, 5)
        self.debug = Config.DEBUG_CRAWLER
        self.scraper = cloudscraper.create_scraper()  # åˆå§‹åŒ– cloudscraper
        # é»˜è®¤çš„nitterå®ä¾‹åˆ—è¡¨ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
        self.default_instances = [
            "https://nitter.net",
            "https://nitter.privacydev.net",
            "https://nitter.1d4.us",
            "https://nitter.moomoo.me",
            "https://nitter.weiler.rocks"
        ]
        
        # åˆ›å»ºå¹¶æ¸…ç†tempç›®å½•
        self.temp_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'temp')
        self.clean_temp_directory()

    def clean_temp_directory(self):
        """æ¸…ç†tempç›®å½•"""
        try:
            if os.path.exists(self.temp_dir):
                self.debug_print("æ¸…ç†tempç›®å½•...")
                shutil.rmtree(self.temp_dir)
            os.makedirs(self.temp_dir, exist_ok=True)
            self.debug_print("âœ… tempç›®å½•å·²å‡†å¤‡å°±ç»ª")
        except Exception as e:
            self.debug_print(f"âŒ æ¸…ç†tempç›®å½•æ—¶å‡ºé”™: {str(e)}")

    def save_html_to_temp(self, url: str, html_content: str):
        """ä¿å­˜HTMLå†…å®¹åˆ°tempç›®å½•"""
        try:
            # ä»URLç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(url)
            filename = f"{parsed_url.netloc}{parsed_url.path}".replace('/', '_') + '.html'
            filepath = os.path.join(self.temp_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            self.debug_print(f"âœ… HTMLå·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            self.debug_print(f"âŒ ä¿å­˜HTMLæ—¶å‡ºé”™: {str(e)}")

    def clean_tweet_text(self, text: str) -> str:
        """æ¸…ç†æ¨æ–‡æ–‡æœ¬ï¼Œç§»é™¤ç”¨æˆ·åå’Œæ—¶é—´ä¿¡æ¯ï¼Œä½†ä¿ç•™ç½®é¡¶æ ‡è¯†"""
        # æ£€æŸ¥å¹¶ä¿ç•™ç½®é¡¶æ ‡è¯†ï¼ˆæ”¯æŒemojiå’Œè‹±æ–‡ä¸¤ç§å½¢å¼ï¼‰
        is_pinned = 'ğŸ“Œ' in text or 'Pinned' in text
        
        # å¦‚æœæ˜¯è‹±æ–‡Pinnedæ ‡è¯†ï¼Œæ›¿æ¢ä¸ºemoji
        text = text.replace('Pinned', 'ğŸ“Œ')
        
        # ç§»é™¤åŒ…å« @ çš„ç”¨æˆ·åä¿¡æ¯
        text = ' '.join([word for word in text.split() if not word.startswith('@')])
        
        # ç§»é™¤æ—¥æœŸä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æ¨æ–‡æœ«å°¾ï¼Œæ ¼å¼å¦‚ Jan 28ï¼‰
        text = ' '.join([word for word in text.split() if not any(month in word for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])])
        
        # ç§»é™¤å…¶ä»–å¸¸è§çš„æ—¶é—´æ ¼å¼ï¼ˆå¦‚ 5h, 2d ç­‰ï¼‰
        text = ' '.join([word for word in text.split() if not (word.endswith(('h', 'd', 'm')) and word[:-1].isdigit())])
        
        # å¦‚æœåŸæ–‡åŒ…å«ç½®é¡¶æ ‡è¯†ï¼Œç¡®ä¿ä¿ç•™
        if is_pinned and not text.startswith('ğŸ“Œ'):
            text = 'ğŸ“Œ ' + text
        
        return text.strip()

    def debug_print(self, message):
        """è°ƒè¯•ä¿¡æ¯æ‰“å°"""
        if self.debug:
            print(f"[Crawler Debug] {message}")

    def get_nitter_instances(self):
        """è·å–æœ€æ–°çš„nitterå®ä¾‹åˆ—è¡¨"""
        try:
            # å°è¯•ä»å¤šä¸ªæ¥æºè·å–å®ä¾‹åˆ—è¡¨
            sources = [
                "https://raw.githubusercontent.com/zedeus/nitter/master/nitter.json",
                "https://raw.githubusercontent.com/zedeus/nitter/master/instances.json",
                "https://github.com/zedeus/nitter/wiki/Instances"
            ]
            
            instances = set()
            
            for source in sources:
                try:
                    self.debug_print(f"å°è¯•ä» {source} è·å–nitterå®ä¾‹åˆ—è¡¨...")
                    response = requests.get(source, headers=self.headers, timeout=10, verify=False)
                    
                    if response.status_code == 200:
                        if source.endswith('.json'):
                            # å¤„ç†JSONæ ¼å¼çš„æº
                            data = response.json()
                            if isinstance(data, dict):
                                instances.update([f"https://{instance}" for instance in data.keys()])
                            elif isinstance(data, list):
                                instances.update([f"https://{instance}" for instance in data])
                        else:
                            # å¤„ç†GitHub wikié¡µé¢
                            soup = BeautifulSoup(response.text, 'html.parser')
                            # è·å–æ‰€æœ‰é“¾æ¥ï¼Œè¿‡æ»¤éhttps://å¼€å¤´çš„é“¾æ¥ï¼Œå¹¶æ’é™¤åŒ…å«githubå’Œssllabsçš„åœ°å€
                            for link in soup.find_all('a', href=True):
                                href = link['href']
                                if (href.startswith('https://') and
                                    'github' not in href and
                                    'ssllabs' not in href):
                                    instances.add(href.rstrip('/'))
                                    
                        self.debug_print(f"ä» {source} æˆåŠŸè·å–å®ä¾‹åˆ—è¡¨")
                        break
                        
                except Exception as e:
                    self.debug_print(f"ä» {source} è·å–å®ä¾‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                    continue
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°ä»»ä½•å®ä¾‹ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨
            if not instances:
                self.debug_print("ä½¿ç”¨é»˜è®¤nitterå®ä¾‹åˆ—è¡¨")
                return self.default_instances
            
            # è¿‡æ»¤æ‰å·²çŸ¥çš„ä¸å¯ç”¨å®ä¾‹
            blacklist = {}  # å¯ä»¥æ·»åŠ å·²çŸ¥ä¸å¯ç”¨çš„å®ä¾‹
            instances = [i for i in instances if not any(b in i for b in blacklist)]
            
            self.debug_print(f"æœ€ç»ˆè·å–åˆ° {len(instances)} ä¸ªnitterå®ä¾‹")
            return list(instances)
            
        except Exception as e:
            self.debug_print(f"è·å–nitterå®ä¾‹åˆ—è¡¨å¤±è´¥: {str(e)}")
            return self.default_instances

    def get_recent_tweets(self, username: str, count: int = 5):
        """è·å–ç”¨æˆ·æœ€è¿‘çš„æ¨æ–‡"""
        try:
            delay = random.uniform(*self.delay_range)
            self.debug_print(f"ç­‰å¾… {delay:.2f} ç§’åå¼€å§‹è·å–æ¨æ–‡...")
            time.sleep(delay)
            
            # è·å–æœ€æ–°çš„nitterå®ä¾‹åˆ—è¡¨
            nitter_instances = self.get_nitter_instances()
            self.debug_print(f"å‡†å¤‡å°è¯•çš„nitterå®ä¾‹æ•°é‡: {len(nitter_instances)}")
            
            tweets = []
            success = False
            
            # éšæœºæ‰“ä¹±å®ä¾‹åˆ—è¡¨é¡ºåº
            random.shuffle(nitter_instances)
            
            for instance in nitter_instances:
                try:
                    url = f"{instance}/{username}"
                    self.debug_print(f"\n=== å°è¯•è®¿é—®URL: {url} ===")
                    
                    # åˆ›å»ºsessionå¹¶è®¾ç½®é‡å®šå‘å¤„ç†
                    session = requests.Session()
                    session.max_redirects = 3
                    
                    # å‘é€è¯·æ±‚ï¼Œç¦ç”¨SSLéªŒè¯
                    self.debug_print("å‘é€è¯·æ±‚...")
                    response = session.get(
                        url,
                        headers=self.headers,
                        timeout=10,
                        verify=False,
                        allow_redirects=True
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°æœ¬åœ°
                    if '127.0.0.1' in response.url or 'localhost' in response.url:
                        self.debug_print(f"âŒ è¢«é‡å®šå‘åˆ°æœ¬åœ°åœ°å€ï¼š{response.url}ï¼Œè·³è¿‡æ­¤URL")
                        continue
                        
                    # å¦‚æœå‘ç”Ÿäº†é‡å®šå‘ï¼Œè¾“å‡ºæœ€ç»ˆURL
                    if response.url != url:
                        self.debug_print(f"âš ï¸ å‘ç”Ÿé‡å®šå‘")
                        self.debug_print(f"åŸå§‹URL: {url}")
                        self.debug_print(f"æœ€ç»ˆURL: {response.url}")
                        
                    self.debug_print(f"è¯·æ±‚çŠ¶æ€ç : {response.status_code}")
                    
                    if response.status_code == 200:
                        self.debug_print("âœ… è¯·æ±‚æˆåŠŸ")
                        
                        # ä¿å­˜HTMLåˆ°tempç›®å½•
                        self.save_html_to_temp(response.url, response.text)
                        
                        soup = BeautifulSoup(response.text, 'html.parser')
                        self.debug_print("æˆåŠŸè§£æé¡µé¢HTML")
                        
                        # è¾“å‡ºé¡µé¢åŸºæœ¬ç»“æ„
                        self.debug_print("\n=== é¡µé¢ç»“æ„åˆ†æ ===")
                        self.debug_print(f"é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
                        error_msg = soup.find(class_=lambda x: x and 'error' in str(x).lower())
                        if error_msg:
                            self.debug_print(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯ä¿¡æ¯: {error_msg.get_text().strip()}")
                        
                        # æŸ¥æ‰¾æ¨æ–‡å®¹å™¨
                        timeline = soup.find('div', class_='timeline')
                        self.debug_print("\n=== Timelineå®¹å™¨æŸ¥æ‰¾ ===")
                        if timeline:
                            self.debug_print("âœ… æ‰¾åˆ°timelineå®¹å™¨")
                            self.debug_print(f"Timelineå®¹å™¨ç±»å: {timeline.get('class', [])}")
                            tweet_items = timeline.find_all(['div', 'article'], class_=['timeline-item', 'tweet-card', 'tweet'])
                            self.debug_print(f"åœ¨timelineä¸­æ‰¾åˆ° {len(tweet_items)} æ¡æ¨æ–‡")
                        else:
                            self.debug_print("âŒ æœªæ‰¾åˆ°timelineå®¹å™¨")
                            self.debug_print("æ£€æŸ¥æ‰€æœ‰divçš„classå±æ€§:")
                            for div in soup.find_all('div', class_=True):
                                self.debug_print(f"å‘ç°divï¼Œç±»å: {div.get('class', [])}")
                            
                            self.debug_print("\nå°è¯•ç›´æ¥æŸ¥æ‰¾æ¨æ–‡...")
                            tweet_items = soup.find_all(['div', 'article'], class_=['timeline-item', 'tweet-card', 'tweet'])
                            
                        self.debug_print(f"\n=== æ¨æ–‡æŸ¥æ‰¾ç»“æœ ===")
                        self.debug_print(f"æ‰¾åˆ° {len(tweet_items)} æ¡åŸå§‹æ¨æ–‡")
                        
                        if not tweet_items:
                            self.debug_print("\n=== ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨ ===")
                            self.debug_print("æœç´¢åŒ…å«tweetæˆ–timeline-itemçš„æ‰€æœ‰å…ƒç´ ")
                            tweet_items = soup.find_all(class_=lambda x: x and any(term in str(x).lower() for term in ['tweet', 'timeline-item']))
                            self.debug_print(f"ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(tweet_items)} æ¡æ¨æ–‡")
                        
                        for item in tweet_items:
                            if len(tweets) >= count:
                                break
                                
                            # è·å–æ¨æ–‡å†…å®¹
                            tweet_content = None
                            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…æ–°å‘ç°çš„ç±»å
                            tweet_content = item.find('div', class_='tweet-content media-body')
                            
                            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç°æœ‰çš„æ¨¡ç³ŠåŒ¹é…æ–¹æ¡ˆ
                            if not tweet_content or not tweet_content.get_text().strip():
                                content_candidates = item.find_all(['div', 'p'], class_=lambda x: x and any(term in str(x).lower() for term in ['content', 'text', 'body']))
                                
                                for candidate in content_candidates:
                                    if candidate.get_text().strip():
                                        tweet_content = candidate
                                        break
                                        
                            if not tweet_content:
                                self.debug_print("è·³è¿‡ä¸€æ¡æ— å†…å®¹çš„æ¨æ–‡")
                                continue
                                
                            content_text = tweet_content.get_text().strip()
                            if not content_text:
                                self.debug_print("è·³è¿‡ä¸€æ¡ç©ºå†…å®¹çš„æ¨æ–‡")
                                continue
                                
                            # æ¸…ç†æ¨æ–‡æ–‡æœ¬
                            content_text = self.clean_tweet_text(content_text)
                            
                            # è·å–æ—¶é—´
                            time_element = item.find(['span', 'a'], class_=lambda x: x and any(term in str(x).lower() for term in ['date', 'time']))
                            tweet_time = "Unknown time"
                            if time_element:
                                tweet_time = time_element.get('title', None) or time_element.get_text().strip()
                            
                            # è·å–ç»Ÿè®¡ä¿¡æ¯
                            stats = {}
                            stats_elements = item.find_all(['span', 'div'], class_=lambda x: x and any(term in str(x).lower() for term in ['stat', 'count', 'activity']))
                            
                            for stat in stats_elements:
                                text = stat.get_text().strip().lower()
                                if 'retweet' in text or 'è½¬æ¨' in text or 'rt' in text:
                                    match = re.search(r'\d+', text)
                                    if match:
                                        stats['retweets'] = match.group()
                                elif 'like' in text or 'å–œæ¬¢' in text:
                                    match = re.search(r'\d+', text)
                                    if match:
                                        stats['likes'] = match.group()
                            
                            # è·å–æ¨æ–‡ID
                            tweet_id = None
                            tweet_link = item.find('a', class_=lambda x: x and any(term in str(x).lower() for term in ['link', 'tweet-link', 'tweet-date']))
                            if tweet_link and 'href' in tweet_link.attrs:
                                href = tweet_link['href']
                                id_match = re.search(r'/status/(\d+)', href)
                                if id_match:
                                    tweet_id = id_match.group(1)
                            
                            tweet = {
                                "text": content_text,
                                "time": tweet_time,
                                "stats": stats,
                                "url": f"https://x.com/{username}/status/{tweet_id}" if tweet_id else None,
                                "order": len(tweets) + 1  # æ·»åŠ é¡ºåºæ ‡è®°
                            }
                            tweets.append(tweet)
                            self.debug_print(f"å·²å¤„ç†ç¬¬ {len(tweets)} æ¡æ¨æ–‡ï¼Œå‘å¸ƒæ—¶é—´: {tweet_time}")
                        
                        if tweets:
                            success = True
                            self.debug_print(f"æˆåŠŸä» {instance} è·å–æ¨æ–‡")
                            break
                        else:
                            self.debug_print("æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆæ¨æ–‡ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹")
                    
                except requests.exceptions.SSLError:
                    self.debug_print(f"SSLé”™è¯¯ï¼Œè·³è¿‡å®ä¾‹ {instance}")
                    continue
                except requests.exceptions.Timeout:
                    self.debug_print(f"è¯·æ±‚è¶…æ—¶ï¼Œè·³è¿‡å®ä¾‹ {instance}")
                    continue
                except requests.exceptions.RequestException as e:
                    self.debug_print(f"è¯·æ±‚é”™è¯¯ï¼Œè·³è¿‡å®ä¾‹ {instance}: {str(e)}")
                    continue
                except Exception as e:
                    self.debug_print(f"æœªçŸ¥é”™è¯¯ï¼Œè·³è¿‡å®ä¾‹ {instance}: {str(e)}")
                    continue
                finally:
                    if 'session' in locals():
                        session.close()
            
            if not success:
                self.debug_print("æ‰€æœ‰å®ä¾‹å‡è·å–å¤±è´¥")
                return []
            
            self.debug_print(f"æœ€ç»ˆè·å–åˆ° {len(tweets)} æ¡æ¨æ–‡")
            return tweets[:count]
            
        except Exception as e:
            self.debug_print(f"è·å–æ¨æ–‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def fetch_page(self, url):
        try:
            # ä½¿ç”¨ cloudscraper å‘é€è¯·æ±‚
            delay = random.uniform(*self.delay_range)
            time.sleep(delay)  # è®¾ç½®éšæœºå»¶è¿Ÿ
            response = self.scraper.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.debug_print(f"è¯·æ±‚å¤±è´¥: {e}")
            return None